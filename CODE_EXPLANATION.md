# –ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–æ–¥–∞ RAG-—Å–∏—Å—Ç–µ–º—ã

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
1. [–û–±—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞](#–æ–±—â–∞—è-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
2. [–ü–æ—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö](#–ø–æ—Ç–æ–∫-–¥–∞–Ω–Ω—ã—Ö)
3. [–î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä –º–æ–¥—É–ª–µ–π](#–¥–µ—Ç–∞–ª—å–Ω—ã–π-—Ä–∞–∑–±–æ—Ä-–º–æ–¥—É–ª–µ–π)
4. [–°–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏](#—Å–ª–æ–∂–Ω—ã–µ-–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏)
5. [–ü—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç—ã](#–ø—Ä–∏–º–µ—Ä—ã-—Ä–∞–±–æ—Ç—ã)

---

## üèóÔ∏è –û–±—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### –°—Ö–µ–º–∞ –≤—ã–∑–æ–≤–æ–≤ (—á—Ç–æ –∫–æ–≥–æ –≤—ã–∑—ã–≤–∞–µ—Ç)

```
app.py (Streamlit UI)
    ‚îÇ
    ‚îú‚îÄ‚îÄ> create_rag_pipeline() [–∏–∑ rag_pipeline.py]
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ‚îÄ> EmbeddingModel() [–∏–∑ embeddings.py]
    ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ> SentenceTransformer (–±–∏–±–ª–∏–æ—Ç–µ–∫–∞)
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ‚îÄ> get_chroma() [–∏–∑ storage.py]
    ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ> ChromaDB (–±–∏–±–ª–∏–æ—Ç–µ–∫–∞)
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îî‚îÄ‚îÄ> get_llm_client() [–∏–∑ llm_client.py]
    ‚îÇ               ‚îî‚îÄ‚îÄ> Ollama (–±–∏–±–ª–∏–æ—Ç–µ–∫–∞)
    ‚îÇ
    ‚îú‚îÄ‚îÄ> extract_text_with_filename() [–∏–∑ docs_parser.py]
    ‚îÇ       ‚îî‚îÄ‚îÄ> read_docx() –∏–ª–∏ read_txt_md()
    ‚îÇ
    ‚îú‚îÄ‚îÄ> prepare_text_for_chunking() [–∏–∑ docs_parser.py]
    ‚îÇ
    ‚îî‚îÄ‚îÄ> split_text() [–∏–∑ chunker.py]
```

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ (–º–æ–¥—É–ª–∏)

| –§–∞–π–ª | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ | –ß—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç |
|------|-----------|----------------|
| `config.py` | –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã) | –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã |
| `embeddings.py` | –†–∞–±–æ—Ç–∞ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ | numpy –º–∞—Å—Å–∏–≤—ã |
| `storage.py` | ChromaDB –∫–ª–∏–µ–Ω—Ç | client, collection |
| `llm_client.py` | –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ | —Å—Ç—Ä–æ–∫–∞ (–æ—Ç–≤–µ—Ç) |
| `docs_parser.py` | –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ | —Ç–µ–∫—Å—Ç |
| `chunker.py` | –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ | —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ |
| `rag_pipeline.py` | –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ RAG | —Å–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º |
| `hybrid_search.py` | –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ | —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ |
| `app.py` | –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å | None (—Ä–µ–Ω–¥–µ—Ä–∏—Ç UI) |

---

## üîÑ –ü–æ—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö

### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞

```
1. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª —á–µ—Ä–µ–∑ Streamlit
   ‚Üì
2. app.py: —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∞–π–ª –≤ data/docs/
   ‚Üì
3. docs_parser.extract_text_with_filename(file_path)
   ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (—Ç–µ–∫—Å—Ç, "–ù–∞–∑–≤–∞–Ω–∏–µ_—Ñ–∞–π–ª–∞")
   ‚Üì
4. docs_parser.prepare_text_for_chunking(text, filename)
   ‚Üí –¥–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫: "–î–æ–∫—É–º–µ–Ω—Ç: –ù–∞–∑–≤–∞–Ω–∏–µ_—Ñ–∞–π–ª–∞\n\n–¢–µ–∫—Å—Ç..."
   ‚Üì
5. chunker.split_text(text, max_length=2000, overlap=200)
   ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç: ["—á–∞–Ω–∫1", "—á–∞–Ω–∫2", ...]
   ‚Üì
6. EmbeddingModel.encode(chunks)
   ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç: numpy array [[0.1, 0.2, ...], [0.3, 0.4, ...]]
   ‚Üì
7. collection.add(documents=chunks, embeddings=embeddings, metadatas=metadata, ids=ids)
   ‚Üí —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ ChromaDB
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –ü–æ–∏—Å–∫ (Query)

```
1. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–≤–æ–¥–∏—Ç –∑–∞–ø—Ä–æ—Å –≤ Streamlit
   ‚Üì
2. app.py: rag.query(user_query, top_k=3)
   ‚Üì
3. RAGPipeline.search_similar(query)
   ‚îú‚îÄ‚îÄ> EmbeddingModel.encode([query])
   ‚îÇ    ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç: numpy array [0.5, 0.6, ...]
   ‚îÇ
   ‚îî‚îÄ‚îÄ> collection.query(query_embeddings=[...], n_results=3)
        ‚Üí ChromaDB –∏—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –≤–µ–∫—Ç–æ—Ä—ã
        ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç: {documents: [...], metadatas: [...], distances: [...]}
   ‚Üì
4. RAGPipeline.format_context(documents)
   ‚Üí —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç: "[–î–æ–∫—É–º–µ–Ω—Ç 1: ...]\n–¢–µ–∫—Å—Ç\n---\n[–î–æ–∫—É–º–µ–Ω—Ç 2: ...]"
   ‚Üì
5. LLMClient.generate_rag_answer(query, context)
   ‚îú‚îÄ‚îÄ> —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç —Å —Å–∏—Å—Ç–µ–º–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π
   ‚îÇ
   ‚îî‚îÄ‚îÄ> ollama.chat(model="qwen2.5:14b", messages=[...])
        ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç: "–û—Ç–≤–µ—Ç –æ—Ç LLM"
   ‚Üì
6. –í–æ–∑–≤—Ä–∞—Ç –≤ app.py
   ‚Üí –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ + –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
```

---

## üîç –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä –º–æ–¥—É–ª–µ–π

### 1. config.py - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```python
import os
BASE_DIR = os.path.dirname(os.path.dirname(__file__))
# __file__ = /home/rinat/GitHub/src/config.py
# os.path.dirname(__file__) = /home/rinat/GitHub/src
# os.path.dirname(dirname) = /home/rinat/GitHub
```

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:**
- `__file__` - –ø—É—Ç—å –∫ —Ç–µ–∫—É—â–µ–º—É —Ñ–∞–π–ª—É
- `os.path.dirname(__file__)` - –ø–æ–ª—É—á–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é (src/)
- –ü–æ–≤—Ç–æ—Ä–Ω—ã–π `dirname` - –ø–æ–¥–Ω–∏–º–∞–µ–º—Å—è –Ω–∞ —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ (–∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞)

**–ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã:**
```python
EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large"
# –ú–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä—ã (384 –∏–∑–º–µ—Ä–µ–Ω–∏—è)

CHUNK_SIZE_TOKENS = 500
# –†–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ (–∫—É—Å–∫–∞ —Ç–µ–∫—Å—Ç–∞) –≤ —Ç–æ–∫–µ–Ω–∞—Ö
# 1 —Ç–æ–∫–µ–Ω ‚âà 0.75 —Å–ª–æ–≤–∞, –∑–Ω–∞—á–∏—Ç ~375 —Å–ª–æ–≤, ~2000 —Å–∏–º–≤–æ–ª–æ–≤

CHUNK_OVERLAP_TOKENS = 50
# –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ (—á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö)

TOP_K = 5
# –°–∫–æ–ª—å–∫–æ —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ø—Ä–∏ –ø–æ–∏—Å–∫–µ

LLM_MODEL_NAME = "qwen2.5:14b-instruct-q4_K_M"
# –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ (14 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)
```

---

### 2. embeddings.py - –†–∞–±–æ—Ç–∞ —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏

```python
from sentence_transformers import SentenceTransformer

class EmbeddingModel:
    def __init__(self, model_name="all-MiniLM-L6-v2"):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ HuggingFace
        self.model = SentenceTransformer(model_name)
        # –ü—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ —Å–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å (~400MB) –≤ ~/.cache/
```

**–ú–µ—Ç–æ–¥ encode:**
```python
def encode(self, texts):
    return self.model.encode(texts, convert_to_numpy=True)
```

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏:**
1. `texts` - –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫: `["—Ç–µ–∫—Å—Ç 1", "—Ç–µ–∫—Å—Ç 2"]`
2. –ú–æ–¥–µ–ª—å –ø—Ä–æ–≥–æ–Ω—è–µ—Ç –∫–∞–∂–¥—ã–π —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ Transformer:
   - –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (—Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å–ª–æ–≤–∞/–ø–æ–¥—Å–ª–æ–≤–∞)
   - –ü—Ä–æ–≥–æ–Ω —á–µ—Ä–µ–∑ BERT-–ø–æ–¥–æ–±–Ω—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å
   - Pooling (—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ) –≤—ã—Ö–æ–¥–æ–≤ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è
3. **–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** numpy –º–∞—Å—Å–∏–≤ —Ñ–æ—Ä–º—ã `(N, 384)`, –≥–¥–µ N - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤

**–ü—Ä–∏–º–µ—Ä:**
```python
embedder = EmbeddingModel("intfloat/multilingual-e5-large")
vectors = embedder.encode(["–ü—Ä–∏–≤–µ—Ç –º–∏—Ä", "Hello world"])
# vectors.shape = (2, 384)
# vectors[0] = [0.123, -0.456, 0.789, ..., 0.234]  # 384 —á–∏—Å–ª–∞
```

**–ó–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ:**
- –¢–µ–∫—Å—Ç—ã —Å –ø–æ—Ö–æ–∂–∏–º —Å–º—ã—Å–ª–æ–º –±—É–¥—É—Ç –∏–º–µ—Ç—å –±–ª–∏–∑–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã
- –ú–æ–∂–µ–º –∏—Å–∫–∞—Ç—å –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ

---

### 3. storage.py - –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö

```python
import chromadb
from src.config import CHROMA_DIR

def get_chroma():
    # –°–æ–∑–¥–∞—ë–º –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç (–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –Ω–∞ –¥–∏—Å–∫)
    client = chromadb.PersistentClient(path=CHROMA_DIR)
    # CHROMA_DIR = "/home/rinat/GitHub/chroma_db"

    # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é "documents"
    collection = client.get_or_create_collection("documents")

    return client, collection
```

**–ß—Ç–æ —Ç–∞–∫–æ–µ ChromaDB:**
- –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –±–∞–∑–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ + –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–¥–µ–∫—Å HNSW (–±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π)
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏

**–ö–∞–∫ —Ö—Ä–∞–Ω—è—Ç—Å—è –¥–∞–Ω–Ω—ã–µ:**
```
chroma_db/
  ‚îî‚îÄ‚îÄ chroma.sqlite3           # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
  ‚îî‚îÄ‚îÄ index/                    # HNSW –∏–Ω–¥–µ–∫—Å
      ‚îî‚îÄ‚îÄ id_to_uuid/
      ‚îî‚îÄ‚îÄ uuid_to_id/
```

**–ú–µ—Ç–æ–¥—ã –∫–æ–ª–ª–µ–∫—Ü–∏–∏:**

1. **–î–æ–±–∞–≤–ª–µ–Ω–∏–µ:**
```python
collection.add(
    documents=["—Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞ 1", "—Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞ 2"],
    embeddings=[[0.1, 0.2, ...], [0.3, 0.4, ...]],  # numpy arrays
    metadatas=[{"doc_id": "123", "chunk_index": 0}, {...}],
    ids=["uuid-1", "uuid-2"]
)
```

2. **–ü–æ–∏—Å–∫:**
```python
results = collection.query(
    query_embeddings=[[0.5, 0.6, ...]],  # –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞
    n_results=5,                          # TOP-5
    where={"active": True}                # —Ñ–∏–ª—å—Ç—Ä –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
)
# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
# {
#   'documents': [["—Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞ 3", "—Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞ 1", ...]],
#   'metadatas': [[{...}, {...}]],
#   'distances': [[0.12, 0.34, ...]]  # –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (0=–∏–¥–µ–Ω—Ç–∏—á–Ω—ã, 2=–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã)
# }
```

---

### 4. llm_client.py - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤

#### –ö–ª–∞—Å—Å LLMClient

**–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
def __init__(self, model_name: str = LLM_MODEL_NAME):
    self.model_name = model_name
    self._verify_model()
```

**–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏:**
```python
def _verify_model(self):
    try:
        models = ollama.list()
        # –ó–∞–ø—Ä–æ—Å –∫ Ollama API: GET http://localhost:11434/api/tags
        # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: {'models': [{'name': 'qwen2.5:14b-instruct-q4_K_M'}, ...]}

        available_models = [m['name'] for m in models.get('models', [])]
        # List comprehension: —Å–æ–∑–¥–∞—ë–º —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π

        if self.model_name not in available_models:
            raise ValueError(f"–ú–æ–¥–µ–ª—å {self.model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    except Exception as e:
        print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {e}")
```

**–ú–µ—Ç–æ–¥ generate:**
```python
def generate(self, prompt: str, system_prompt: str = None,
             max_tokens: int = 1024, temperature: float = 0.7) -> str:

    messages = []

    # 1. –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if system_prompt:
        messages.append({'role': 'system', 'content': system_prompt})

    # 2. –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å
    messages.append({'role': 'user', 'content': prompt})

    # 3. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Ollama
    response = ollama.chat(
        model=self.model_name,
        messages=messages,
        options={
            'num_predict': max_tokens,  # –º–∞–∫—Å. –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
            'temperature': temperature   # —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å (0=–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π, 1=–∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π)
        }
    )

    # 4. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
    return response['message']['content']
```

**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Ollama:**
1. –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –Ω–∞ –ø–æ—Ä—Ç—É 11434
2. –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ –ø–∞–º—è—Ç—å (–∑–∞–Ω–∏–º–∞–µ—Ç ~8GB RAM –¥–ª—è qwen2.5:14b-q4)
3. –ü—Ä–∏ –≤—ã–∑–æ–≤–µ `ollama.chat()`:
   - –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç POST –∑–∞–ø—Ä–æ—Å –∫ http://localhost:11434/api/chat
   - –ü–µ—Ä–µ–¥–∞—ë—Ç –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π
   - –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –∞–≤—Çoregressive (—Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º)
   - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç

**–ú–µ—Ç–æ–¥ generate_rag_answer:**
```python
def generate_rag_answer(self, query: str, context: str, max_tokens: int = 1024) -> str:

    # 1. –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏)
    system_prompt = """–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ø–æ–∏—Å–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.

–í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
1. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
2. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ ‚Äî —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ "–í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
3. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
4. –û—Ç–≤–µ—á–∞–π —á–µ—Ç–∫–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ
"""

    # 2. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç (–∫–æ–Ω—Ç–µ–∫—Å—Ç + –≤–æ–ø—Ä–æ—Å)
    prompt = f"""–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:
{context}

---

–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
{query}

---

–û–¢–í–ï–¢ (–∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤—ã—à–µ):"""

    # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –Ω–∏–∑–∫–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π (–¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏)
    return self.generate(
        prompt=prompt,
        system_prompt=system_prompt,
        max_tokens=max_tokens,
        temperature=0.3  # –ù–∏–∑–∫–∞—è = –±–æ–ª–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π –æ—Ç–≤–µ—Ç
    )
```

**–ó–∞—á–µ–º temperature=0.3:**
- –ü—Ä–∏ temperature=0: –º–æ–¥–µ–ª—å –≤—Å–µ–≥–¥–∞ –≤—ã–±–∏—Ä–∞–µ—Ç —Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ)
- –ü—Ä–∏ temperature=1: –º–æ–¥–µ–ª—å –±–æ–ª–µ–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–∞, –º–æ–∂–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä–æ–≤–∞—Ç—å
- 0.3 - –±–∞–ª–∞–Ω—Å: —Ç–æ—á–Ω–æ—Å—Ç—å + –Ω–µ–º–Ω–æ–≥–æ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏

---

### 5. chunker.py - –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏

```python
def split_text(text: str, max_length=500, overlap=50):
    chunks = []
    start = 0

    while start < len(text):
        end = min(len(text), start + max_length)
        chunk = text[start:end]
        chunks.append(chunk)
        start += max_length - overlap  # –°–¥–≤–∏–≥–∞–µ–º –Ω–∞ (max_length - overlap)

    return chunks
```

**–ü–æ—à–∞–≥–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä:**
```python
text = "0123456789ABCDEFGHIJ"  # 20 —Å–∏–º–≤–æ–ª–æ–≤
chunks = split_text(text, max_length=10, overlap=3)

# –ò—Ç–µ—Ä–∞—Ü–∏—è 1:
#   start=0, end=10, chunk="0123456789", start_next=0+10-3=7
# –ò—Ç–µ—Ä–∞—Ü–∏—è 2:
#   start=7, end=17, chunk="789ABCDEFG", start_next=7+10-3=14
# –ò—Ç–µ—Ä–∞—Ü–∏—è 3:
#   start=14, end=20, chunk="EFGHIJ", start_next=14+10-3=21 > len(text) ‚Üí –≤—ã—Ö–æ–¥

# –†–µ–∑—É–ª—å—Ç–∞—Ç:
# ["0123456789", "789ABCDEFG", "EFGHIJ"]
#   ^^^                 ^^^         –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ "789"
```

**–ó–∞—á–µ–º overlap (–ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ):**
- –ß—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö —á–∞–Ω–∫–æ–≤
- –ï—Å–ª–∏ –≤–∞–∂–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ, –æ–Ω–æ –ø–æ–ø–∞–¥—ë—Ç –≤ –æ–±–∞ —á–∞–Ω–∫–∞

**–ü–æ—á–µ–º—É max_length=500 —Ç–æ–∫–µ–Ω–æ–≤ ‚Üí 2000 —Å–∏–º–≤–æ–ª–æ–≤:**
- 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
- 500 —Ç–æ–∫–µ–Ω–æ–≤ √ó 4 = 2000 —Å–∏–º–≤–æ–ª–æ–≤

---

### 6. docs_parser.py - –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

**–ß—Ç–µ–Ω–∏–µ .txt –∏ .md:**
```python
def read_txt_md(file_path: str) -> str:
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()
```
- –ü—Ä–æ—Å—Ç–æ–µ —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Ü–µ–ª–∏–∫–æ–º
- `encoding="utf-8"` - –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã

**–ß—Ç–µ–Ω–∏–µ .docx:**
```python
from docx import Document

def read_docx(file_path: str) -> str:
    doc = Document(file_path)
    # Document - –æ–±—ä–µ–∫—Ç python-docx, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Word –¥–æ–∫—É–º–µ–Ω—Ç

    text = "\n".join(p.text for p in doc.paragraphs)
    # List comprehension + generator expression
    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö —á–µ—Ä–µ–∑ \n

    return text
```

**–†–∞–∑–±–æ—Ä –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:**
```python
# –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–π –∫–æ–¥ –±–µ–∑ list comprehension:
paragraphs_texts = []
for p in doc.paragraphs:
    paragraphs_texts.append(p.text)
text = "\n".join(paragraphs_texts)
```

**–ß—Ç–æ —Ç–∞–∫–æ–µ doc.paragraphs:**
- –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ Word –¥–æ–∫—É–º–µ–Ω—Ç–µ
- –ö–∞–∂–¥—ã–π `p` - –æ–±—ä–µ–∫—Ç Paragraph —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º `.text`

**–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è extract_text:**
```python
def extract_text(file_path: str) -> str:
    ext = os.path.splitext(file_path)[1].lower()
    # os.path.splitext("/path/to/file.DOCX") = ("/path/to/file", ".DOCX")
    # [1] –±–µ—Ä—ë–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ, .lower() –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É

    if ext == ".txt" or ext == ".md":
        return read_txt_md(file_path)
    elif ext == ".docx":
        return read_docx(file_path)
    else:
        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {ext}")
```

**–° –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ñ–∞–π–ª–∞:**
```python
def extract_text_with_filename(file_path: str) -> Tuple[str, str]:
    text = extract_text(file_path)

    filename = os.path.basename(file_path)
    # os.path.basename("/path/to/–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è.docx") = "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è.docx"

    filename_without_ext = os.path.splitext(filename)[0]
    # os.path.splitext("–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è.docx") = ("–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è", ".docx")
    # [0] –±–µ—Ä—ë–º —á–∞—Å—Ç—å –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è

    return text, filename_without_ext
```

**–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞:**
```python
def prepare_text_for_chunking(text: str, filename_without_ext: str) -> str:
    header = f"–î–æ–∫—É–º–µ–Ω—Ç: {filename_without_ext}\n\n"
    return header + text
```

**–ó–∞—á–µ–º –¥–æ–±–∞–≤–ª—è—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫:**
- –ß—Ç–æ–±—ã –≤ –∫–∞–∂–¥–æ–º —á–∞–Ω–∫–µ –±—ã–ª–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- LLM –±—É–¥–µ—Ç –∑–Ω–∞—Ç—å, –∏–∑ –∫–∞–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
- –£–ª—É—á—à–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ

---

### 7. hybrid_search.py - –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫

#### –ó–∞—á–µ–º –Ω—É–∂–µ–Ω –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫:

**–ü—Ä–æ–±–ª–µ–º–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞:**
- –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏—â–µ—Ç "–æ—à–∏–±–∫–∞ 409", –∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–∞–ø–∏—Å–∞–Ω–æ "error 409"
- –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –º–æ–∂–µ—Ç –Ω–µ –Ω–∞–π—Ç–∏ (—Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–∏)

**–†–µ—à–µ–Ω–∏–µ - BM25:**
- –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º (TF-IDF –ø–æ–¥–æ–±–Ω—ã–π)
- –•–æ—Ä–æ—à –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤, —á–∏—Å–µ–ª, –∫–æ–¥–æ–≤

**–ì–∏–±—Ä–∏–¥–Ω—ã–π = Semantic + BM25**

#### –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:
```python
def tokenize_russian(text: str) -> List[str]:
    import re
    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ \b\w+\b:
    #   \b - –≥—Ä–∞–Ω–∏—Ü–∞ —Å–ª–æ–≤–∞ (word boundary)
    #   \w+ - –æ–¥–∏–Ω –∏–ª–∏ –±–æ–ª–µ–µ –±—É–∫–≤–µ–Ω–Ω–æ-—Ü–∏—Ñ—Ä–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    # –ü—Ä–∏–º–µ—Ä: "–û—à–∏–±–∫–∞-409!" ‚Üí ["–æ—à–∏–±–∫–∞", "409"]

    tokens = re.findall(r'\b\w+\b', text.lower())
    return tokens
```

**–ü—Ä–∏–º–µ—Ä:**
```python
text = "–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤ –≤—Ä—É—á–Ω—É—é. –í–µ—Ä—Å–∏—è 1.2"
tokens = tokenize_russian(text)
# ["–∑–∞–≥—Ä—É–∑–∫–∞", "—Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤", "–≤—Ä—É—á–Ω—É—é", "–≤–µ—Ä—Å–∏—è", "1", "2"]
```

#### –ö–ª–∞—Å—Å HybridSearcher:

**–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
def __init__(self, documents: List[str], document_ids: List[str]):
    self.documents = documents
    self.document_ids = document_ids

    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    tokenized_docs = [tokenize_russian(doc) for doc in documents]
    # [["–∑–∞–≥—Ä—É–∑–∫–∞", "—Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤"], ["–æ—à–∏–±–∫–∞", "409"], ...]

    # –°–æ–∑–¥–∞—ë–º BM25 –∏–Ω–¥–µ–∫—Å
    self.bm25 = BM25Okapi(tokenized_docs)
    # BM25Okapi - –∫–ª–∞—Å—Å –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ rank-bm25
    # –°—Ç—Ä–æ–∏—Ç –æ–±—Ä–∞—Ç–Ω—ã–π –∏–Ω–¥–µ–∫—Å: —Å–ª–æ–≤–æ ‚Üí [–¥–æ–∫1, –¥–æ–∫3, ...]
```

**–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ BM25:**
```python
def search_bm25(self, query: str, top_k: int = 10) -> List[Dict]:
    # 1. –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
    tokenized_query = tokenize_russian(query)
    # "–æ—à–∏–±–∫–∞ 409" ‚Üí ["–æ—à–∏–±–∫–∞", "409"]

    # 2. –í—ã—á–∏—Å–ª—è–µ–º BM25 scores –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    scores = self.bm25.get_scores(tokenized_query)
    # numpy array: [0.0, 2.3, 0.1, 4.5, ...]

    # 3. –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –ø–æ —É–±—ã–≤–∞–Ω–∏—é scores
    top_indices = np.argsort(scores)[::-1][:top_k]
    # np.argsort([0.0, 2.3, 0.1, 4.5]) = [0, 2, 1, 3]
    # [::-1] —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º: [3, 1, 2, 0]
    # [:top_k] –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ top_k

    # 4. –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = []
    for idx in top_indices:
        if scores[idx] > 0:  # –¢–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
            results.append({
                'doc_id': self.document_ids[idx],
                'text': self.documents[idx],
                'bm25_score': float(scores[idx])
            })

    return results
```

**–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:**
```python
@staticmethod
def combine_scores(semantic_results, bm25_results,
                   semantic_weight=0.5, bm25_weight=0.5):

    # 1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è scores (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ [0, 1])
    def normalize_scores(results, score_key):
        scores = [r[score_key] for r in results]
        min_score, max_score = min(scores), max(scores)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: (x - min) / (max - min)
        # –ü—Ä–∏–º–µ—Ä: [10, 20, 30] ‚Üí [0.0, 0.5, 1.0]
        for r in results:
            r[f'{score_key}_norm'] = (r[score_key] - min_score) / (max_score - min_score)
        return results

    # 2. –î–ª—è semantic: –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º distance –≤ score
    for r in semantic_results:
        r['semantic_score'] = 1 - r.get('distance', 0)
        # distance=0.2 ‚Üí score=0.8

    semantic_results = normalize_scores(semantic_results, 'semantic_score')
    bm25_results = normalize_scores(bm25_results, 'bm25_score')

    # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ doc_id –≤ —Å–ª–æ–≤–∞—Ä—å
    combined = {}

    for r in semantic_results:
        doc_id = r.get('id') or r.get('doc_id')
        combined[doc_id] = {
            'semantic_norm': r.get('semantic_score_norm', 0),
            'bm25_norm': 0,  # –ø–æ–∫–∞ 0, –æ–±–Ω–æ–≤–∏–º –µ—Å–ª–∏ –µ—Å—Ç—å –≤ bm25_results
            # ... –¥—Ä—É–≥–∏–µ –ø–æ–ª—è
        }

    for r in bm25_results:
        doc_id = r['doc_id']
        if doc_id in combined:
            combined[doc_id]['bm25_norm'] = r.get('bm25_score_norm', 0)
        else:
            # –î–æ–∫—É–º–µ–Ω—Ç —Ç–æ–ª—å–∫–æ –≤ BM25, –Ω–µ –≤ semantic
            combined[doc_id] = {
                'semantic_norm': 0,
                'bm25_norm': r.get('bm25_score_norm', 0),
                # ...
            }

    # 4. –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π hybrid_score
    for doc_id in combined:
        combined[doc_id]['hybrid_score'] = (
            semantic_weight * combined[doc_id]['semantic_norm'] +
            bm25_weight * combined[doc_id]['bm25_norm']
        )
        # hybrid = 0.5 √ó 0.8 + 0.5 √ó 0.6 = 0.7

    # 5. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ hybrid_score (–æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É)
    results = sorted(combined.values(), key=lambda x: x['hybrid_score'], reverse=True)

    return results
```

**–ó–∞—á–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è:**
- Semantic distance: –æ—Ç 0 –¥–æ 2 (–∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ)
- BM25 score: –æ—Ç 0 –¥–æ ~100 (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
- –ë–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ BM25 "–∑–∞–¥–∞–≤–∏—Ç" semantic
- –ü–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: –æ–±–∞ –æ—Ç 0 –¥–æ 1, –º–æ–∂–Ω–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å

---

### 8. rag_pipeline.py - –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞

#### –ö–ª–∞—Å—Å RAGPipeline:

**–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
def __init__(self, embedding_model_name: str = EMBEDDING_MODEL_NAME, top_k: int = TOP_K):
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG pipeline...")

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    self.embedding_model = EmbeddingModel(embedding_model_name)
    # –ó–∞–≥—Ä—É–∂–∞–µ—Ç ~400MB –º–æ–¥–µ–ª—å –∏–∑ HuggingFace

    # 2. –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ ChromaDB
    self.client, self.collection = get_chroma()
    # –û—Ç–∫—Ä—ã–≤–∞–µ—Ç /chroma_db/chroma.sqlite3

    # 3. –°–æ–∑–¥–∞—ë–º LLM –∫–ª–∏–µ–Ω—Ç
    self.llm_client = get_llm_client()
    # –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω

    self.top_k = top_k
    print("‚úÖ RAG pipeline –≥–æ—Ç–æ–≤")
```

**–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:**
```python
def search_similar(self, query: str, top_k: int = None, filter_active: bool = True):
    if top_k is None:
        top_k = self.top_k

    # 1. –°–æ–∑–¥–∞—ë–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
    query_embedding = self.embedding_model.encode([query])[0].tolist()
    # encode([query]) –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç numpy array —Ñ–æ—Ä–º—ã (1, 384)
    # [0] –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç ‚Üí array —Ñ–æ—Ä–º—ã (384,)
    # .tolist() –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ Python [0.1, 0.2, ...]

    # 2. –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
    where_filter = {"active": True} if filter_active else None
    # –ï—Å–ª–∏ True, —Ç–æ –∏—â–µ–º —Ç–æ–ª—å–∫–æ —Å—Ä–µ–¥–∏ active=True –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    # 3. –ü–æ–∏—Å–∫ –≤ ChromaDB
    results = self.collection.query(
        query_embeddings=[query_embedding],  # —Å–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤ (–º–æ–∂–µ–º –∏—Å–∫–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ)
        n_results=top_k,
        where=where_filter if where_filter else {}
    )

    # 4. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    documents = []
    if results['documents'] and len(results['documents']) > 0:
        # results['documents'] = [["—Ç–µ–∫—Å—Ç1", "—Ç–µ–∫—Å—Ç2", ...]]  # –≤–ª–æ–∂–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫!
        # results['metadatas'] = [[{meta1}, {meta2}, ...]]
        # results['distances'] = [[0.12, 0.34, ...]]

        for i in range(len(results['documents'][0])):
            doc = {
                'text': results['documents'][0][i],
                'metadata': results['metadatas'][0][i] if results['metadatas'] else {},
                'distance': results['distances'][0][i] if results['distances'] else None,
                'id': results['ids'][0][i] if results['ids'] else None
            }
            documents.append(doc)

    return documents
```

**–ü–æ—á–µ–º—É –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏:**
- ChromaDB –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–∫–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å—Ä–∞–∑—É
- `query_embeddings=[[vec1], [vec2]]` ‚Üí –¥–≤–∞ –∑–∞–ø—Ä–æ—Å–∞
- –û—Ç–≤–µ—Ç: `{'documents': [[—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã_–¥–ª—è_vec1], [—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã_–¥–ª—è_vec2]]}`
- –£ –Ω–∞—Å –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å ‚Üí –±–µ—Ä—ë–º `[0]`

**–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:**
```python
def format_context(self, documents: List[Dict]) -> Tuple[str, List[Dict]]:
    if not documents:
        return "–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.", []

    context_parts = []
    sources = []

    for i, doc in enumerate(documents, 1):
        # enumerate(documents, 1) ‚Üí (1, doc1), (2, doc2), ...

        text = doc['text']
        metadata = doc.get('metadata', {})

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
        source_info = {
            'index': i,
            'filename': metadata.get('filename', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç'),
            'doc_id': metadata.get('doc_id', ''),
            'distance': doc.get('distance', 0.0)
        }
        sources.append(source_info)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_part = f"""[–î–æ–∫—É–º–µ–Ω—Ç {i}: {source_info['filename']}]
{text}
"""
        context_parts.append(context_part)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
    context = "\n---\n".join(context_parts)

    return context, sources
```

**–ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:**
```
[–î–æ–∫—É–º–µ–Ω—Ç 1: –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤]
–î–æ–∫—É–º–µ–Ω—Ç: –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤

–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤ –≤—Ä—É—á–Ω—É—é...

---

[–î–æ–∫—É–º–µ–Ω—Ç 2: –û—à–∏–±–∫–∏ –ö–ö–ú]
–î–æ–∫—É–º–µ–Ω—Ç: –û—à–∏–±–∫–∏ –ö–ö–ú

–û—à–∏–±–∫–∞ 409: –Ω–µ–≤–µ—Ä–Ω—ã–π –∫–æ–¥ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏...
```

**–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ query:**
```python
def query(self, user_query: str, top_k: int = None) -> Dict:
    print(f"\nüîç –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {user_query}")

    # 1. –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    documents = self.search_similar(user_query, top_k=top_k)

    if not documents:
        return {
            'answer': "–í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
            'context': "",
            'sources': [],
            'documents': []
        }

    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")

    # 2. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    context, sources = self.format_context(documents)

    # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM
    print("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
    answer = self.llm_client.generate_rag_answer(
        query=user_query,
        context=context
    )

    print("‚úÖ –û—Ç–≤–µ—Ç –≥–æ—Ç–æ–≤")

    return {
        'answer': answer,
        'context': context,
        'sources': sources,
        'documents': documents
    }
```

---

### 9. app.py - Streamlit –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

**–ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ RAG pipeline:**
```python
@st.cache_resource
def get_rag_pipeline():
    return create_rag_pipeline()
```

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç @st.cache_resource:**
- –ü—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –µ—ë –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç
- –ü—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã–∑–æ–≤–∞—Ö –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
- –ù–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë—Ç –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
- –í–∞–∂–Ω–æ –¥–ª—è —Ç—è–∂—ë–ª—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ (–º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π)

**–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è:**
```python
def main():
    st.title("üìö RAG –ü–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG pipeline (–æ–¥–∏–Ω —Ä–∞–∑ –±–ª–∞–≥–æ–¥–∞—Ä—è –∫–µ—à—É)
    rag = get_rag_pipeline()

    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
    with st.sidebar:
        stats = rag.get_stats()
        st.metric("–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤", stats['total_chunks'])
        top_k = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", 1, 10, 3)

    # –í–∫–ª–∞–¥–∫–∏
    tab1, tab2, tab3 = st.tabs(["üîç –ü–æ–∏—Å–∫", "üìÑ –ó–∞–≥—Ä—É–∑–∫–∞", "üìä –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π"])

    with tab1:
        query = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å:")

        if st.button("üîç –ù–∞–π—Ç–∏"):
            if not query:
                st.warning("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å")
            else:
                with st.spinner("–ü–æ–∏—Å–∫..."):
                    result = rag.query(query, top_k=top_k)

                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
                    st.success(result['answer'])

                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                    for source in result['sources']:
                        with st.expander(f"üìÑ {source['filename']}"):
                            st.text(result['documents'][source['index'] - 1]['text'])
```

**–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞:**
```python
with tab2:
    uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª", type=['docx', 'md', 'txt'])

    if st.button("üì§ –ó–∞–≥—Ä—É–∑–∏—Ç—å"):
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        file_path = f"data/docs/{uploaded_file.name}"
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        text, filename_without_ext = extract_text_with_filename(file_path)

        # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
        text_with_header = prepare_text_for_chunking(text, filename_without_ext)

        # 4. –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
        chunks = split_text(text_with_header, max_length=2000, overlap=200)

        # 5. –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embedding_model = EmbeddingModel(EMBEDDING_MODEL_NAME)
        embeddings = embedding_model.encode(chunks)

        # 6. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        doc_id = str(uuid.uuid4())  # —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID

        for i, chunk in enumerate(chunks):
            chunk_id = f"{doc_id}_chunk_{i}"
            metadata = {
                'doc_id': doc_id,
                'filename': uploaded_file.name,
                'chunk_index': i,
                'total_chunks': len(chunks),
                'active': True,
                # ...
            }

        # 7. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ ChromaDB
        collection.add(
            documents=chunks,
            embeddings=embeddings.tolist(),
            metadatas=metadatas,
            ids=chunk_ids
        )

        st.success("üéâ –î–æ–∫—É–º–µ–Ω—Ç –¥–æ–±–∞–≤–ª–µ–Ω!")
        st.cache_resource.clear()  # –û—á–∏—Å—Ç–∫–∞ –∫–µ—à–∞
```

---

## üß© –°–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏

### 1. List Comprehension

**–ë–∞–∑–æ–≤—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å:**
```python
result = [–≤—ã—Ä–∞–∂–µ–Ω–∏–µ for —ç–ª–µ–º–µ–Ω—Ç in –∏—Ç–µ—Ä–∏—Ä—É–µ–º–æ–µ if —É—Å–ª–æ–≤–∏–µ]
```

**–ü—Ä–∏–º–µ—Ä—ã –∏–∑ –∫–æ–¥–∞:**

```python
# 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π
available_models = [m['name'] for m in models.get('models', [])]

# –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç:
available_models = []
for m in models.get('models', []):
    available_models.append(m['name'])
```

```python
# 2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
tokenized_docs = [tokenize_russian(doc) for doc in documents]

# –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç:
tokenized_docs = []
for doc in documents:
    tokenized_docs.append(tokenize_russian(doc))
```

```python
# 3. –° —É—Å–ª–æ–≤–∏–µ–º
results = [r for r in all_results if r['score'] > 0.5]

# –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç:
results = []
for r in all_results:
    if r['score'] > 0.5:
        results.append(r)
```

### 2. Generator Expression

```python
text = "\n".join(p.text for p in doc.paragraphs)
```

**–û—Ç–ª–∏—á–∏–µ –æ—Ç list comprehension:**
- `[p.text for p in doc.paragraphs]` - —Å–æ–∑–¥–∞—ë—Ç —Å–ø–∏—Å–æ–∫ –≤ –ø–∞–º—è—Ç–∏
- `(p.text for p in doc.paragraphs)` - –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä (–≤—ã—á–∏—Å–ª—è–µ—Ç –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é)
- `"\n".join()` –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∏—Ç–µ—Ä–∏—Ä—É–µ–º–æ–µ, –ø–æ—ç—Ç–æ–º—É –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä

### 3. Conditional Expression (—Ç–µ—Ä–Ω–∞—Ä–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä)

```python
where_filter = {"active": True} if filter_active else None

# –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç:
if filter_active:
    where_filter = {"active": True}
else:
    where_filter = None
```

```python
metadata = doc.get('metadata', {})
# –ï—Å–ª–∏ –∫–ª—é—á 'metadata' –µ—Å—Ç—å - –≤–µ—Ä–Ω—ë—Ç –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏–µ
# –ï—Å–ª–∏ –Ω–µ—Ç - –≤–µ—Ä–Ω—ë—Ç {} (–ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å)
```

### 4. Slicing –∏ —à–∞–≥

```python
top_indices = np.argsort(scores)[::-1][:top_k]
```

**–†–∞–∑–±–æ—Ä:**
```python
arr = [1, 2, 3, 4, 5]

arr[::-1]     # –†–∞–∑–≤–æ—Ä–æ—Ç: [5, 4, 3, 2, 1]
arr[::2]      # –ö–∞–∂–¥—ã–π –≤—Ç–æ—Ä–æ–π: [1, 3, 5]
arr[1:4]      # –° 1 –ø–æ 3: [2, 3, 4]
arr[:3]       # –ü–µ—Ä–≤—ã–µ 3: [1, 2, 3]
```

### 5. Enumerate

```python
for i, doc in enumerate(documents, 1):
    print(f"–î–æ–∫—É–º–µ–Ω—Ç {i}")
```

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:**
```python
documents = ['doc1', 'doc2', 'doc3']

# enumerate(documents) ‚Üí (0, 'doc1'), (1, 'doc2'), (2, 'doc3')
# enumerate(documents, 1) ‚Üí (1, 'doc1'), (2, 'doc2'), (3, 'doc3')

for i, doc in enumerate(documents, 1):
    # i=1, doc='doc1'
    # i=2, doc='doc2'
    # i=3, doc='doc3'
```

### 6. Lambda —Ñ—É–Ω–∫—Ü–∏–∏

```python
results = sorted(combined.values(), key=lambda x: x['hybrid_score'], reverse=True)
```

**–ß—Ç–æ —Ç–∞–∫–æ–µ lambda:**
```python
# Lambda - –∞–Ω–æ–Ω–∏–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è (—Ñ—É–Ω–∫—Ü–∏—è –±–µ–∑ –∏–º–µ–Ω–∏)
lambda x: x['hybrid_score']

# –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç:
def get_score(x):
    return x['hybrid_score']

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
sorted(combined.values(), key=get_score, reverse=True)
```

**–ó–∞—á–µ–º lambda:**
- –ö–æ—Ä–æ—Ç–∫–∞—è –∑–∞–ø–∏—Å—å –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
- –ù–µ –Ω—É–∂–Ω–æ –¥–∞–≤–∞—Ç—å –∏–º—è

### 7. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä (with)

```python
with open(file_path, "r", encoding="utf-8") as f:
    return f.read()
```

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:**
```python
# –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç:
f = open(file_path, "r", encoding="utf-8")
try:
    result = f.read()
    return result
finally:
    f.close()  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –∑–∞–∫—Ä–æ–µ—Ç—Å—è, –¥–∞–∂–µ –µ—Å–ª–∏ –æ—à–∏–±–∫–∞
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ with:**
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç —Ñ–∞–π–ª
- –î–∞–∂–µ –µ—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞
- –ö–æ—Ä–æ—á–µ –∏ –ø–æ–Ω—è—Ç–Ω–µ–µ

### 8. F-strings (—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫)

```python
header = f"–î–æ–∫—É–º–µ–Ω—Ç: {filename_without_ext}\n\n"
```

**–†–∞–∑–ª–∏—á–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã:**
```python
name = "–ò–≤–∞–Ω"
age = 25

# 1. F-string (—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π, Python 3.6+)
text = f"–ú–µ–Ω—è –∑–æ–≤—É—Ç {name}, –º–Ω–µ {age} –ª–µ—Ç"

# 2. format() (—Å—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–±)
text = "–ú–µ–Ω—è –∑–æ–≤—É—Ç {}, –º–Ω–µ {} –ª–µ—Ç".format(name, age)

# 3. % –æ–ø–µ—Ä–∞—Ç–æ—Ä (–æ—á–µ–Ω—å —Å—Ç–∞—Ä—ã–π)
text = "–ú–µ–Ω—è –∑–æ–≤—É—Ç %s, –º–Ω–µ %d –ª–µ—Ç" % (name, age)
```

**–í—ã—Ä–∞–∂–µ–Ω–∏—è –≤ f-string:**
```python
f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {2 + 2}"  # "–†–µ–∑—É–ª—å—Ç–∞—Ç: 4"
f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {1 - distance:.2%}"  # "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 85.30%"
```

### 9. –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤

```python
client, collection = get_chroma()
```

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:**
```python
# get_chroma() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç tuple: (client_obj, collection_obj)
result = get_chroma()
# result = (client_obj, collection_obj)

# –†–∞—Å–ø–∞–∫–æ–≤–∫–∞:
client, collection = result
# client = result[0]
# collection = result[1]
```

**–î—Ä—É–≥–∏–µ –ø—Ä–∏–º–µ—Ä—ã:**
```python
# –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞
a, b, c = [1, 2, 3]

# –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π
first, _, third = [1, 2, 3]  # _ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

# –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ —Å –æ—Å—Ç–∞—Ç–∫–æ–º
first, *rest = [1, 2, 3, 4]
# first = 1, rest = [2, 3, 4]
```

### 10. Dictionary .get() —Å default

```python
metadata.get('filename', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç')
```

**–†–∞–∑–Ω–∏—Ü–∞:**
```python
metadata = {'doc_id': '123'}

# 1. –ü—Ä—è–º–æ–µ –æ–±—Ä–∞—â–µ–Ω–∏–µ - –æ—à–∏–±–∫–∞ –µ—Å–ª–∏ –Ω–µ—Ç –∫–ª—é—á–∞
filename = metadata['filename']  # KeyError!

# 2. get() - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –µ—Å–ª–∏ –Ω–µ—Ç –∫–ª—é—á–∞
filename = metadata.get('filename')  # None

# 3. get() —Å default - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç default –µ—Å–ª–∏ –Ω–µ—Ç –∫–ª—é—á–∞
filename = metadata.get('filename', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π')  # '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π'
```

---

## üìä –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç—ã

### –ü—Ä–∏–º–µ—Ä 1: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞

**–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:**
```
–§–∞–π–ª: "–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤.docx"
–°–æ–¥–µ—Ä–∂–∏–º–æ–µ:
"–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤ –≤—Ä—É—á–Ω—É—é –∑–∞–π–¥–∏—Ç–µ –Ω–∞ ftp://...
–°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –Ω–∞ —Ñ–ª–µ—à–∫—É.
–ù–∞ –º–∞–≥–∞–∑–∏–Ω–µ –ø–æ–ª–æ–∂–∏—Ç–µ –≤ –∫–æ—Ä–µ–Ω—å –¥–∏—Å–∫–∞ C:."
```

**–ü—Ä–æ—Ü–µ—Å—Å:**

1. **–ü–∞—Ä—Å–∏–Ω–≥** (docs_parser.py):
```python
text, filename = extract_text_with_filename("–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤.docx")
# text = "–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤ –≤—Ä—É—á–Ω—É—é..."
# filename = "–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤"
```

2. **–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞**:
```python
text_with_header = prepare_text_for_chunking(text, filename)
# "–î–æ–∫—É–º–µ–Ω—Ç: –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤\n\n–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤..."
```

3. **–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏** (chunker.py):
```python
chunks = split_text(text_with_header, max_length=2000, overlap=200)
# [
#   "–î–æ–∫—É–º–µ–Ω—Ç: –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤\n\n–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏...",
#   "...—Å–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –Ω–∞ —Ñ–ª–µ—à–∫—É...",
#   "...–ø–æ–ª–æ–∂–∏—Ç–µ –≤ –∫–æ—Ä–µ–Ω—å –¥–∏—Å–∫–∞ C:."
# ]
```

4. **–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤** (embeddings.py):
```python
embeddings = embedding_model.encode(chunks)
# numpy array —Ñ–æ—Ä–º—ã (3, 384)
# [
#   [0.123, -0.456, 0.789, ...],  # 384 —á–∏—Å–ª–∞ –¥–ª—è —á–∞–Ω–∫–∞ 1
#   [0.234, -0.567, 0.890, ...],  # 384 —á–∏—Å–ª–∞ –¥–ª—è —á–∞–Ω–∫–∞ 2
#   [0.345, -0.678, 0.901, ...]   # 384 —á–∏—Å–ª–∞ –¥–ª—è —á–∞–Ω–∫–∞ 3
# ]
```

5. **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ ChromaDB** (storage.py):
```python
collection.add(
    documents=chunks,
    embeddings=embeddings.tolist(),
    metadatas=[
        {'doc_id': 'uuid-123', 'chunk_index': 0, 'filename': '–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤.docx'},
        {'doc_id': 'uuid-123', 'chunk_index': 1, 'filename': '–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤.docx'},
        {'doc_id': 'uuid-123', 'chunk_index': 2, 'filename': '–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤.docx'}
    ],
    ids=['uuid-123_chunk_0', 'uuid-123_chunk_1', 'uuid-123_chunk_2']
)
```

### –ü—Ä–∏–º–µ—Ä 2: –ü–æ–∏—Å–∫

**–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:**
```
"–ö–∞–∫ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ –≤—Ä—É—á–Ω—É—é?"
```

**–ü—Ä–æ—Ü–µ—Å—Å:**

1. **–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞**:
```python
query_embedding = embedding_model.encode(["–ö–∞–∫ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ –≤—Ä—É—á–Ω—É—é?"])
# array —Ñ–æ—Ä–º—ã (1, 384) ‚Üí –±–µ—Ä—ë–º [0] ‚Üí (384,)
# [0.789, -0.123, 0.456, ...]
```

2. **–ü–æ–∏—Å–∫ –≤ ChromaDB**:
```python
results = collection.query(
    query_embeddings=[[0.789, -0.123, ...]],
    n_results=3
)

# ChromaDB –≤—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É query_embedding –∏ –≤—Å–µ–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –≤ –ë–î
# –§–æ—Ä–º—É–ª–∞: distance = 1 - (A ¬∑ B) / (||A|| √ó ||B||)
# –ì–¥–µ A ¬∑ B - —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤

# –†–µ–∑—É–ª—å—Ç–∞—Ç:
# {
#   'documents': [["–î–æ–∫—É–º–µ–Ω—Ç: –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤\n\n–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏...", ...]],
#   'metadatas': [[{'filename': '–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤.docx', ...}]],
#   'distances': [[0.12, 0.34, 0.56]]  # —á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ø–æ—Ö–æ–∂–µ–µ
# }
```

3. **–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞**:
```python
context = """[–î–æ–∫—É–º–µ–Ω—Ç 1: –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤.docx]
–î–æ–∫—É–º–µ–Ω—Ç: –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤

–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤ –≤—Ä—É—á–Ω—É—é –∑–∞–π–¥–∏—Ç–µ –Ω–∞ ftp://...

---

[–î–æ–∫—É–º–µ–Ω—Ç 2: ...]
..."""
```

4. **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM**:
```python
prompt = f"""–ö–û–ù–¢–ï–ö–°–¢:
{context}

–í–û–ü–†–û–°: –ö–∞–∫ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ –≤—Ä—É—á–Ω—É—é?

–û–¢–í–ï–¢:"""

answer = llm_client.generate(prompt, system_prompt="–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç...")
# "–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤ –≤—Ä—É—á–Ω—É—é –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ:
#  1. –ó–∞–π—Ç–∏ –Ω–∞ ftp://sps-holding.ru/kb_y/SPRmanually/
#  2. –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã –Ω–∞ —Ñ–ª–µ—à–∫—É
#  3. –ù–∞ –º–∞–≥–∞–∑–∏–Ω–µ –ø–æ–ª–æ–∂–∏—Ç—å –≤ –∫–æ—Ä–µ–Ω—å –¥–∏—Å–∫–∞ C:"
```

5. **–í–æ–∑–≤—Ä–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞**:
```python
return {
    'answer': "–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤ –≤—Ä—É—á–Ω—É—é –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ...",
    'sources': [
        {'filename': '–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤.docx', 'distance': 0.12}
    ],
    'documents': [...]
}
```

---

## üéØ –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è

### 1. –ü–æ—á–µ–º—É –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:
- –¢–µ–∫—Å—Ç—ã —Å –ø–æ—Ö–æ–∂–∏–º —Å–º—ã—Å–ª–æ–º –∏–º–µ—é—Ç –±–ª–∏–∑–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã
- –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –º–∏–ª–ª–∏–æ–Ω–∞—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
- –ü–æ–Ω–∏–º–∞–µ—Ç —Å–∏–Ω–æ–Ω–∏–º—ã, –ø–∞—Ä–∞—Ñ—Ä–∞–∑—ã, –∫–æ–Ω—Ç–µ–∫—Å—Ç

### 2. –ü–æ—á–µ–º—É –Ω—É–∂–µ–Ω —á–∞–Ω–∫–∏–Ω–≥:
- –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ (512 —Ç–æ–∫–µ–Ω–æ–≤)
- –ú–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏ = —Ç–æ—á–Ω–µ–µ –ø–æ–∏—Å–∫ (–º–µ–Ω—å—à–µ —à—É–º–∞)
- –ë–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏ = –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

### 3. –ü–æ—á–µ–º—É overlap –≤–∞–∂–µ–Ω:
- –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ —á–∞–Ω–∫–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞–∑—Ä–µ–∑–∞–Ω–æ
- –° overlap –æ–Ω–æ –ø–æ–ø–∞–¥—ë—Ç –≤ –æ–±–∞ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–∞

### 4. –ü–æ—á–µ–º—É —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∫—Ä–∏—Ç–∏—á–µ–Ω:
- –ë–µ–∑ –Ω–µ–≥–æ LLM –º–æ–∂–µ—Ç "–≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä–æ–≤–∞—Ç—å" (–ø—Ä–∏–¥—É–º—ã–≤–∞—Ç—å —Ñ–∞–∫—Ç—ã)
- –ü—Ä–æ–º–ø—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç
- Temperature=0.3 —Å–Ω–∏–∂–∞–µ—Ç –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å

### 5. –ü–æ—á–µ–º—É ChromaDB, –∞ –Ω–µ –æ–±—ã—á–Ω–∞—è –ë–î:
- –û–±—ã—á–Ω–∞—è –ë–î: –ø–æ–∏—Å–∫ –ø–æ —Ç–æ—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é (SQL LIKE)
- ChromaDB: –ø–æ–∏—Å–∫ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã (HNSW) –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ ANN

---

## üìù –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ö–µ–º–∞

```
–ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–¨
    ‚Üì (–≤–≤–æ–¥–∏—Ç –∑–∞–ø—Ä–æ—Å)
STREAMLIT UI (app.py)
    ‚Üì (–≤—ã–∑—ã–≤–∞–µ—Ç)
RAG PIPELINE (rag_pipeline.py)
    ‚îú‚îÄ‚Üí EMBEDDINGS (embeddings.py)
    ‚îÇ   ‚îî‚îÄ‚Üí query ‚Üí vector [0.1, 0.2, ...]
    ‚îÇ
    ‚îú‚îÄ‚Üí CHROMA DB (storage.py)
    ‚îÇ   ‚îî‚îÄ‚Üí vector ‚Üí TOP-K –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    ‚îÇ
    ‚îî‚îÄ‚Üí LLM CLIENT (llm_client.py)
        ‚îî‚îÄ‚Üí –∫–æ–Ω—Ç–µ–∫—Å—Ç + –∑–∞–ø—Ä–æ—Å ‚Üí –û–¢–í–ï–¢
            ‚Üì
        OLLAMA (qwen2.5:14b)
            ‚Üì
–ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–¨
    ‚Üì (–≤–∏–¥–∏—Ç –æ—Ç–≤–µ—Ç + –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
```

---

–≠—Ç–æ –ø–æ–ª–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –∫–æ–¥–∞! –ï—Å–ª–∏ –Ω—É–∂–Ω—ã –ø–æ—è—Å–Ω–µ–Ω–∏—è –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —á–∞—Å—Ç—è–º - —Å–ø—Ä–∞—à–∏–≤–∞–π!
